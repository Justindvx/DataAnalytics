> cbcsubset=subset(cbc,select=c(M,F,R,FirstPurch,ArtBks,ItalAtlas,ItalArt))
> library(infotheo)
> cbcsubset = discretize(cbcsubset, disc="equalfreq", nbins=4 )
> head(cbcsubset)
  M F R FirstPurch ArtBks ItalAtlas ItalArt
1 4 2 3          3      1         1       1
2 1 2 1          1      1         1       1
3 2 4 4          4      1         1       1
4 3 1 1          1      1         1       1
5 3 1 2          1      1         1       1
6 2 2 1          1      1         1       1
> cbcsubset2=subset(cbc,select=c(Gender,Florence))
> cbcMain=cbind(cbcsubset,cbcsubset2)
> head(cbcMain)
  M F R FirstPurch ArtBks ItalAtlas ItalArt Gender Florence
1 4 2 3          3      1         1       1      1        0
2 1 2 1          1      1         1       1      0        0
3 2 4 4          4      1         1       1      1        0
4 3 1 1          1      1         1       1      1        0
5 3 1 2          1      1         1       1      1        0
6 2 2 1          1      1         1       1      1        0
> attach(cbcMain)
The following object(s) are masked from 'cbcMain (position 6)':

    ArtBks, F, FirstPurch, Florence, Gender, ItalArt, ItalAtlas, M, R
The following object(s) are masked from 'cbcMain (position 8)':

    ArtBks, F, Florence, Gender, ItalArt, ItalAtlas, M
The following object(s) are masked from 'cbcMain (position 9)':

    ArtBks, F, Florence, Gender, ItalArt, ItalAtlas, M
The following object(s) are masked from 'cbcMain (position 33)':

    ArtBks, F, Florence, Gender, ItalArt, ItalAtlas, M
The following object(s) are masked from 'package:base':

    F
> cbcMain$M=as.factor(M)
> cbcMain$F=as.factor(F)
> cbcMain$R=as.factor(R)
> cbcMain$FirstPurch=as.factor(FirstPurch)
> cbcMain$ArtBks=as.factor(ArtBks)
> cbcMain$ItalAtlas=as.factor(ItalAtlas)
> cbcMain$ItalArt=as.factor(ItalArt)
> cbcMain$Florence=as.factor(Florence)
> cbcMain$Gender=as.factor(Gender)
> summary(cbcMain)
 M        F        R        FirstPurch ArtBks   ItalAtlas ItalArt  Gender  
 1:1004   1:1227   1:1176   1:1013     1:3108   1:3870    1:3827   0:1182  
 2:1005   2:1203   2: 998   2: 994     4: 892   4: 130    4: 173   1:2818  
 3: 995   3: 624   3:1011   3:1032                                         
 4: 996   4: 946   4: 815   4: 961                                         
 Florence
 0:3662  
 1: 338  
         
         
> train=sample(1:4000,3000)
>  cbc_train=cbcMain[train,]
>  test=(1:4000)[-train]
>  cbc_test=cbcMain[test,]
>  table(cbc_train$Florence)

   0    1 
2746  254 
>   table(cbc_test$Florence)

  0   1 
916  84 
> library(e1071)
> #Naive Bayes using training data
> modelnb = naiveBayes(Florence ~ ., data = cbc_train)
> table(cbc_train$Florence, predict(modelnb, cbc_train))
   
       0    1
  0 2739    7
  1  253    1
> pred=predict(modelnb, cbc_train)
> a=table(cbc_train$Florence, pred)t 
Error: unexpected symbol in "a=table(cbc_train$Florence, pred)t"
> e_nbtr=(a[1,2]+a[2,1])/(a[1,1]+a[2,2]+a[1,2]+a[2,1])*100
> pred=predict(modelnb, cbc_train)
> a=table(cbc_train$Florence, pred)
> e_nbtr=(a[1,2]+a[2,1])/(a[1,1]+a[2,2]+a[1,2]+a[2,1])*100
> e_nbtr
[1] 8.666667
> r_nbtr=(a[2,2])/(a[2,1]+a[2,2])*100
> r_nbtr
[1] 0.3937008
> pred=predict(modelnb, cbc_test)
> a=table(cbc_test$Florence, pred)
> e_nbte=(a[1,2]+a[2,1])/(a[1,1]+a[2,2]+a[1,2]+a[2,1])*100
> r_nbte=(a[2,2])/(a[2,1]+a[2,2])*100
> r_nbte
[1] 0
> names(cbcMain)
[1] "M"          "F"          "R"          "FirstPurch" "ArtBks"     "ItalAtlas" 
[7] "ItalArt"    "Gender"     "Florence"  
> cbcMains=cbcMain[,c(1,2,3,4,5,6,7,8,9)]
> cbcMains=SMOTE(Florence~.,cbcMains,prec.over=400)
> dim(cbcMains)
[1] 2366    9
>  rownames(cbcMains)=as.character(seq(1:2366))
> table(cbcMains$Florence)

   0    1 
1352 1014 
> train=sample(1:2366,2000)
> train=sample(1:2366,1800)
> cbcs_train=cbcMains[train,]
> test=(1:2366)[-train]
> cbcs_test=cbcMains[test,]
> table(cbcs_train$Florence)

   0    1 
1042  758 
> table(cbcs_test$Florence)

  0   1 
310 256 
> modelnbs = naiveBayes(Florence ~ ., data = cbcs_train)
> table(cbcs_train$Florence, predict(modelnbs, cbcs_train))
   
      0   1
  0 871 171
  1 316 442
> a=table(cbcs_train$Florence, predict(modelnbs, cbcs_train))
> r_nbstr=(a[2,2])/(a[2,1]+a[2,2])*100
> r_nbstr
[1] 58.31135
> a=table(cbcs_test$Florence, predict(modelnbs, cbcs_test))
>  r_nbste=(a[2,2])/(a[2,1]+a[2,2])*100
>  r_nbste
[1] 61.71875
> table(cbc_test$Florence, predict(modelnbs, cbc_test))
   
      0   1
  0 769 147
  1  63  21
> a=table(cbc_test$Florence, predict(modelnbs, cbc_test))
> r_nbsote=(a[2,2])/(a[2,1]+a[2,2])*100
>  r_nbsote
[1] 25
> library(rpart)
> modeldtcart <- rpart(Florence ~ ., data = cbc_train, method="class", cp=0.001)
> prettyTree(modeldtcart)
Error in plot.rpart(t, uniform = uniform, branch = branch, margin = margin,  : 
  fit is not a tree, just a root
> printcp(modeldtcart)

Classification tree:
rpart(formula = Florence ~ ., data = cbc_train, method = "class", 
    cp = 0.001)

Variables actually used in tree construction:
character(0)

Root node error: 254/3000 = 0.084667

n= 3000 

  CP nsplit rel error
1  0      0         1
> modeldtscart <- rpart(Florence ~ ., data = cbcs_train, method="class", cp=0.001)
> prettyTree(modeldtscart)
> printcp(modeldtscart)

Classification tree:
rpart(formula = Florence ~ ., data = cbcs_train, method = "class", 
    cp = 0.001)

Variables actually used in tree construction:
[1] ArtBks     F          FirstPurch Gender     ItalArt    ItalAtlas  M         
[8] R         

Root node error: 758/1800 = 0.42111

n= 1800 

          CP nsplit rel error  xerror     xstd
1  0.2546174      0   1.00000 1.00000 0.027635
2  0.0290237      1   0.74538 0.74538 0.025975
3  0.0230871      2   0.71636 0.76385 0.026145
4  0.0211082      4   0.67018 0.74274 0.025950
5  0.0118734      5   0.64908 0.68470 0.025354
6  0.0098945      6   0.63720 0.67546 0.025252
7  0.0092348      8   0.61741 0.68074 0.025311
8  0.0072559      9   0.60818 0.67018 0.025192
9  0.0052770     11   0.59367 0.64380 0.024881
10 0.0039578     13   0.58311 0.62401 0.024635
11 0.0033924     15   0.57520 0.62401 0.024635
12 0.0032982     22   0.55145 0.62533 0.024652
13 0.0019789     24   0.54485 0.63325 0.024752
14 0.0013193     28   0.53694 0.64116 0.024849
15 0.0010000     32   0.53166 0.66359 0.025116
> fit2=prune(modeldtscart,cp=0.25)
> prettyTree(fit2)
> printcp(fit2)

Classification tree:
rpart(formula = Florence ~ ., data = cbcs_train, method = "class", 
    cp = 0.001)

Variables actually used in tree construction:
[1] ItalArt

Root node error: 758/1800 = 0.42111

n= 1800 

       CP nsplit rel error  xerror     xstd
1 0.25462      0   1.00000 1.00000 0.027635
2 0.25000      1   0.74538 0.74538 0.025975
> modeldtc45= J48(Florence ~ ., data = cbc_train)
> summary(modeldtc45)

=== Summary ===

Correctly Classified Instances        2746               91.5333 %
Incorrectly Classified Instances       254                8.4667 %
Kappa statistic                          0     
Mean absolute error                      0.155 
Root mean squared error                  0.2784
Relative absolute error                 99.8519 %
Root relative squared error            100      %
Coverage of cases (0.95 level)         100      %
Mean rel. region size (0.95 level)     100      %
Total Number of Instances             3000     

=== Confusion Matrix ===

    a    b   <-- classified as
 2746    0 |    a = 0
  254    0 |    b = 1
> a=table(cbc_train$Florence, predict(modeldtc45,cbc_train))
> r_c45tr=(a[2,2])/(a[2,1]+a[2,2])*100
> r_c45tr
[1] 0
> a=table(cbcs_train$Florence, predict(modeldtsc45,cbcs_train))
Error in model.frame.default(delete.response(terms(object)), newdata,  : 
  variable lengths differ (found for 'ChildBks')
In addition: Warning message:
'newdata' had 1800 rows but variable(s) found have 4000 rows 
> r_c45tr_cbcs=(a[2,2])/(a[2,1]+a[2,2])*100
> r_c45tr_cbcs
[1] 0
> modeldtsc45= J48(Florence ~ ., data = cbcs_train)
> summary(modeldtsc45)

=== Summary ===

Correctly Classified Instances        1374               76.3333 %
Incorrectly Classified Instances       426               23.6667 %
Kappa statistic                          0.489 
Mean absolute error                      0.3472
Root mean squared error                  0.4166
Relative absolute error                 71.205  %
Root relative squared error             84.3842 %
Coverage of cases (0.95 level)         100      %
Mean rel. region size (0.95 level)      98.25   %
Total Number of Instances             1800     

=== Confusion Matrix ===

   a   b   <-- classified as
 968  74 |   a = 0
 352 406 |   b = 1
> a=table(cbcs_train$Florence, predict(modeldtsc45,cbcs_train))
> r_c45tr_cbcs=(a[2,2])/(a[2,1]+a[2,2])*100
> r_c45tr_cbcs
[1] 53.56201
> a=table(cbc_test$Florence, predict(modeldtc45,cbc_test))
> r_c45te=(a[2,2])/(a[2,1]+a[2,2])*100
> r_c45te
[1] 0
> a=table(cbcs_test$Florence, predict(modeldtsc45,cbcs_test))
> r_c45te_cbcs=(a[2,2])/(a[2,1]+a[2,2])*100
> r_c45te_cbcs
[1] 53.90625
> set.seed=1234
> modelrf=randomForest(Florence~., data=cbc_train, ntree=700)
> plot(modelrf)
> importance(modelrf)
           MeanDecreaseGini
M                 11.140728
F                  8.701599
R                 11.241071
FirstPurch         7.374382
ArtBks             7.250859
ItalAtlas          3.373942
ItalArt            4.043119
Gender             5.271813
>  modelrfs=randomForest(Florence~., data=cbcs_train, ntree=700)
> plot(modelrfs)
> importance(modelrfs)
           MeanDecreaseGini
M                  30.13715
F                  45.20293
R                  37.74556
FirstPurch         40.60703
ArtBks             41.94968
ItalAtlas          38.43006
ItalArt            87.91916
Gender             19.08320
> a=table(cbc_train$Florence, predict(modelrf, cbc_train))
> r_rftr=(a[2,2])/(a[2,1]+a[2,2])*100
> r_rftr
[1] 1.181102
>  a=table(cbc_train$Florence, predict(modelrf, cbc_train))
> r_rftr=(a[2,2])/(a[2,1]+a[2,2])*100
> r_rftr
[1] 1.181102
> a=table(cbcs_train$Florence, predict(modelrfs, cbcs_train))
> r_rftr_cbcs=(a[2,2])/(a[2,1]+a[2,2])*100
> r_rftr_cbcs
[1] 58.44327
> a=table(cbc_test$Florence, predict(modelrf,cbc_test))
>  r_rfte=(a[2,2])/(a[2,1]+a[2,2])*100
>   r_rfte
[1] 0
> a=table(cbcs_test$Florence, predict(modelrfs,cbcs_test))
>  r_rfte_cbcs=(a[2,2])/(a[2,1]+a[2,2])*100
>  r_rfte_cbcs
[1] 54.29688
> modelab <- AdaBoostM1(Florence ~ .,cbc_train, control=Weka_control(I=100))
> table(cbc_train$Florence, predict(modelab))
   
       0    1
  0 2746    0
  1  254    0
> modelabs <- AdaBoostM1(Florence ~ .,cbcs_train, control=Weka_control(I=100))
> table(cbcs_train$Florence, predict(modelabs))
   
      0   1
  0 892 150
  1 338 420
> a=table(cbc_train$Florence, predict(modelab, cbc_train))
> r_abtr=(a[2,2])/(a[2,1]+a[2,2])*100
> r_abtr
[1] 0
> a=table(cbcs_train$Florence, predict(modelabs, cbcs_train))
>  r_abtr_cbcs=(a[2,2])/(a[2,1]+a[2,2])*100
>  r_abtr_cbcs
[1] 55.40897
>  a=table(cbc_test$Florence, predict(modelab, cbc_test))
>   r_abte=(a[2,2])/(a[2,1]+a[2,2])*100
>   r_abte
[1] 0
>  a=table(cbcs_test$Florence, predict(modelabs, cbcs_test))
>   r_abte_cbcs=(a[2,2])/(a[2,1]+a[2,2])*100
>   r_abte_cbcs
[1] 60.54688
> recall_comp=matrix(c(r_nbtr, r_nbte, r_nbstr, r_nbste, r_c45tr, r_c45te,r_c45tr_cbcs,r_c45te_cbcs, r_rftr, r_rfte,r_rftr_cbcs,r_rfte_cbcs, r_abtr, r_abte,r_abtr_cbcs,r_abte_cbcs), nrow=1)
> colnames(recall_comp)=c('NB_Tr', 'NB_TE', 'NBS_TR', 'NBS_TE', 'C4.5TR', 'C4.5TE','C4.5TR_SMOTE','C4.5TE_SMOTE', 'RFTR', 'RFTE','RFTR_SMOTE','RFTE_SMOTE', 'ABTR', 'ABTE','ABTR_SMOTE','ABTE_SMOTE')
> recall_comp
         NB_Tr NB_TE   NBS_TR   NBS_TE C4.5TR C4.5TE C4.5TR_SMOTE C4.5TE_SMOTE
[1,] 0.3937008     0 58.31135 61.71875      0      0     53.56201     53.90625
         RFTR RFTE RFTR_SMOTE RFTE_SMOTE ABTR ABTE ABTR_SMOTE ABTE_SMOTE
[1,] 1.181102    0   58.44327   54.29688    0    0   55.40897   60.54688
> recall_comp
         NB_Tr NB_TE   NBS_TR   NBS_TE C4.5TR C4.5TE C4.5TR_SMOTE C4.5TE_SMOTE     RFTR RFTE RFTR_SMOTE RFTE_SMOTE ABTR ABTE ABTR_SMOTE ABTE_SMOTE
[1,] 0.3937008     0 58.31135 61.71875      0      0     53.56201     53.90625 1.181102    0   58.44327   54.29688    0    0   55.40897   60.54688
> 
